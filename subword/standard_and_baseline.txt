'''
This result was obtained with the facebook public module and 1 billion truncated wiki dataset.
The parameters are as follows.
Because of the randomness of window size, negative sampling, and etc, a standard was established as an average value after five experiments.


1 billion wiki dataset has the 218,316 words in the vocabulary.
And the numbers below refer to the number of valid questions excluding oov in syntactic and semantic analogy tasks (at Mikolov, 2013a).
valid semantic: 506/8869
valid syntactic: 9076/10675


In order to obtain new baselines different from the paper because of the changed dataset from 2006 wiki to latest truncated wiki dataset, the word2vec model that I implemented earlier was trained with a wiki dataset.
'''



./fasttext skipgram -input data/fil9 -output result/fil9 -minn 3 -maxn 6 -dim 300 -epoch 5 -lr 0.05 -thread 80
1 try
syntactic acc: 61.2 [%]
semantic acc: 47.7 [%]

2 try
syntactic acc: 62.8 [%]
semantic acc: 47.2 [%]

3 try
syntactic acc: 60.8 [%]
semantic acc: 47.5 [%]

4 try
syntactic acc: 62.3 [%]
semantic acc: 46.2 [%]

5 try
syntactic acc: 62.4 [%]
semantic acc: 46.6 [%]

------------ avg ------------
syntactic acc: 61.9 [%]
semantic acc: 47.1 [%]





===============================================  Word2Vec baselines ===============================================
python script/run_train.py --model CBOW --activation NEG --window_size 5 --least_freq 5 --negative 5 --max_epoch 3 --sub_t 1e-05 --lr 0.05 --hidden_dim 300 --seq_len 35
syntactic acc: 30.8 [%]
semantic acc: 56.3 [%]

python script/run_train.py --model SkipGram --activation NEG --window_size 5 --least_freq 5 --negative 5 --max_epoch 3 --sub_t 1e-05 --lr 0.025 --hidden_dim 300 --seq_len 35
syntactic acc: 36.3 [%]
semantic acc: 50.8 [%]



python script/run_train.py --model CBOW --activation NEG --window_size 5 --least_freq 5 --negative 5 --max_epoch 3 --sub_t 1e-04 --lr 0.05 --hidden_dim 300 --seq_len 35
syntactic acc: 43.4 [%]
semantic acc: 67.6 [%]

python script/run_train.py --model SkipGram --activation NEG --window_size 5 --least_freq 5 --negative 5 --max_epoch 3 --sub_t 1e-04 --lr 0.025 --hidden_dim 300 --seq_len 35
syntactic acc: 44.4 [%]
semantic acc: 64.0 [%]



python script/run_train.py --model CBOW --activation NEG --window_size 5 --least_freq 5 --negative 5 --max_epoch 5 --sub_t 1e-04 --lr 0.05 --hidden_dim 300 --seq_len 35
syntactic acc: 48.3 [%]
semantic acc: 70.0 [%]

python script/run_train.py --model SkipGram --activation NEG --window_size 5 --least_freq 5 --negative 5 --max_epoch 5 --sub_t 1e-04 --lr 0.025 --hidden_dim 300 --seq_len 35



